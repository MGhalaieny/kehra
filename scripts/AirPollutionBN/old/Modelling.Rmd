---
title: "Modelling"
author: "Claudia Vitolo"
date: "4th January 2016"
output: html_document
---

Locate stations in England, active between 1998 and 2013
```{r}
# library(ggmap)
# p <- get_map("England", zoom = 6)
# ggmap(p) + geom_point(data=siteDetails, aes(x=longitude, y=latitude), color="red", size=3, alpha=0.5) + 
#   geom_point(data=x, aes(x=Longitude, y=Latitude), color="blue", size=3, alpha=0.5)
stations <- readRDS("data/Pollution/openairStations.rds")
library(leaflet)
leaflet(data = stations) %>% addTiles() %>% addCircleMarkers(radius = 5, popup = ~Name)
```

# Statistical analysis at 1 location: London Harlington (UKA00472)
Data is available from ("http://uk-air.defra.gov.uk/data/flat_files?site_id=HRL"), from 2004 to 2015. 

```{r}
rm(list = ls())
closeAllConnections()

require(INLA)
require(maptools)
require(lattice)
library(manipulate)

# install.packages("openair")
library(openair)
HRL <- importAURN(site = "HRL", year = 2004:2015) # Harlington
MY1 <- importAURN(site = "MY1", year = 2004:2015) # Marylebone Road
mySites <- c(HRL, MY1)

# DEFRA Daily Air Quality Index (DAQI)
## the labels - same for all species 
labels <- c("1 - Low", "2 - Low", "3 - Low", "4 - Moderate", "5 - Moderate", 
            "6 - Moderate", "7 - High", "8 - High", "9 - High", "10 - Very High")
o3.breaks <-c(0, 34, 66, 100, 121, 141, 160, 188, 214, 240, 500) 
no2.breaks <- c(0, 67, 134, 200, 268, 335, 400, 468, 535, 600, 1000) 
pm10.breaks <- c(0, 17, 34, 50, 59, 67, 75, 84, 92, 100, 1000) 
pm25.breaks <- c(0, 12, 24, 35, 42, 47, 53, 59, 65, 70, 1000)

manipulate(calendarPlot(eval(parse(text=site)), year = x, 
                        pollutant = "pm10", annotate = "value", 
                        labels = labels, breaks = pm10.breaks,
                        lim = pm10.breaks[4], col.lim = c("black", "orange"),
                        statistic = "mean", cols = "jet"),
           x = slider(2004, 2015),
           site = picker("HRL", "MY1"))

## no2 index example 
manipulate(calendarPlot(eval(parse(text=site)), year = x, 
                        pollutant = "no2", annotate = "value", 
                        labels = labels, breaks = no2.breaks,
                        lim = no2.breaks[4], col.lim = c("black", "orange"),
                        statistic = "max", cols = "jet"),
           x = slider(2004, 2015),
           site = picker("HRL", "MY1"))
## for PM10 or PM2.5 we need the daily mean concentration 
manipulate(calendarPlot(eval(parse(text=site)), year = x, 
                        pollutant = "pm10", annotate = "value", 
                        labels = labels, breaks = pm10.breaks,
                        lim = pm10.breaks[4], col.lim = c("black", "orange"),
                        statistic = "mean", cols = "jet"),
           x = slider(2004, 2015),
           site = picker("HRL", "MY1"))
manipulate(calendarPlot(eval(parse(text=site)), year = x, 
                        pollutant = "pm25", annotate = "value", 
                        labels = labels, breaks = pm25.breaks,
                        lim = pm25.breaks[4], col.lim = c("black", "orange"),
                        statistic = "mean", cols = "jet"),
           x = slider(2004, 2015),
           site = picker("HRL", "MY1"))
## for ozone, need the rolling 8-hour mean 
manipulate(calendarPlot(rollingMean(eval(parse(text=site)), 
                                    pollutant = "o3", hours = 8),
                        year = x, 
                        pollutant = "rolling8o3", annotate = "value", 
                        labels = labels, breaks = o3.breaks,
                        lim = o3.breaks[4], col.lim = c("black", "orange"),
                        statistic = "max", cols = "jet"),
           x = slider(2004, 2015),
           site = picker("HRL", "MY1"))
```

Let's try a summary plot for MY1

```{r}
summaryPlot(MY1[,c("date", "co", "pm10", "no", "no2", "nox", "o3", "site", "code", "pm2.5", "ws", "wd")])
```

Let's try a summary plot for HRL

```{r}
summaryPlot(HRL[,c("date", "co", "pm10", "no", "no2", "nox", "o3", "site", "code", "pm2.5", "ws", "wd")])
```

Correlation matrix + dendogram at HRL
```{r}
corPlot(HRL[,c("date", "co", "pm10", "no", "no2", "nox", "o3", "site", "code", "pm2.5", "ws", "wd")], dendrogram = TRUE)
```

Correlation matrix + dendogram for all AURN stations in 2014
```{r}
df <- readRDS("data/Pollution/allPollutantsClima.rds")
corPlot(df, dendrogram = TRUE)
```

Explore table
```{r}
names(df)
# "Date", "id", "Longitude", "Latitude", "Site", "Region", "Z",
# "PM10", "PM2p5", "CO", "O3", "NO2", "SO2", "TEMP", "WS", "WD", "HMIX", "PREC"
head(df)

df1 <- na.omit(df[,c("Longitude", "Latitude", "Z",
                     "PM10", "PM2p5", "CO", "O3", "NO2", "SO2", 
                     "TEMP", "WS", "WD", "HMIX", "PREC")])

# There are some NAs in columns 9:13, remove rows with all NAs
temp <- df[,c("PM10", "PM2p5", "CO", "O3", "NO2", "SO2")]
df2 <- df[rowSums(is.na(temp)) != ncol(temp),]
# Keep only complete cases (rows with no NAs)
df3 <- df[complete.cases(temp),]

df0 <- df[complete.cases(df[,c("PM10", "TEMP", "PREC")]), 
          c("PM10", "TEMP", "WS", "WD", "HMIX", "PREC")]
```

# BAYESIAN NETWORKS
Install dependencies from the Bioconductor and CRAN repos
```{r}
# For PC algorithm
# sudo apt-get install libgmp3-dev
# install.packages("gmp")
# install.packages("pcalg")
library(pcalg)
require(Rgraphviz)

# For bnlearn
# Bioconductor
# source("https://bioconductor.org/biocLite.R")
# biocLite("graph")
# biocLite("RBGL")
# biocLite("Rgraphviz")

# CRAN
# install.packages(c("Matrix", "igraph", "Rcpp"))
# install.packages("gRbase")
# install.packages("gRain")
# install.packages("bnlearn")
```

Load the full dataset for the UK.

```{r}
df <- readRDS("~/kehra/data/EnglandDB_1979_2014.rds")

# How many stations?
length(unique(df$site)) # 154

# How many stations measured O3? For how many years?
length(unique(df$site[!is.na(df$o3)])) # 93
sort(unique(df$Year[!is.na(df$o3)])) # 36 years

# How many stations measured CO? For how many years?
length(unique(df$site[!is.na(df$co)])) # 84
sort(unique(df$Year[!is.na(df$co)])) # 36 years

# How many stations measured NO2? For how many years?
length(unique(df$site[!is.na(df$no2)])) # 137
sort(unique(df$Year[!is.na(df$no2)])) # 36 years

# How many stations measured PM10? For how many years?
length(unique(df$site[!is.na(df$pm10)])) # 78
sort(unique(df$Year[!is.na(df$pm10)])) # 23 years

# How many stations measured PM2.5? For how many years?
length(unique(df$site[!is.na(df$pm2.5)])) # 56
sort(unique(df$Year[!is.na(df$pm2.5)])) # 17 years

# How many stations measured SO2? For how many years?
length(unique(df$site[!is.na(df$so2)])) # 56
sort(unique(df$Year[!is.na(df$so2)])) # 17 years
```


```{r}
str(df)
na_count <- sapply(df, function(y) sum(length(which(is.na(y)))))
data.frame(names(df), as.numeric(na_count))
# Change labels
# names(df)[c(13:15, 38, 42)] <- c("LAT", "LON", "ALT", "C60", "L60")
df$la_region_id <- as.numeric(as.character(df$la_region_id))
df$Year <- as.numeric(as.character(df$Year))
df$Month <- as.numeric(as.character(df$Month))
df$Day <- as.numeric(as.character(df$Day))
df$Time <- as.numeric(as.character(df$Time))
```

# TEST THE PC ALGORITHM FOR CAUSAL RELATIONSHIPS
```{r}
# Start SIMPLE!
df00 <- df[!is.na(df$CVD60), c(12, 15:22, 28, 30, 33, 34, 38)]
# df00 <- df[!is.na(df$CVD60), c(1:4, 12, 13:22, 28, 30, 33, 34, 38, 42)]
names(df00) <- c("Y", "M", "D", "T", "RID", 
                 "LAT", "LON", "ALT", "WS", "WD", "O3",
                 "SO2", "CO", "PM10", "PM25", "NO2", "TP", "BLH", "R", 
                 "C60", "L60")
# df00 <- df[!is.na(df$CVD60), c(4, 12, 15:22, 28, 30, 33, 34, 38)]
str(df00)
suffStat <- list(C = cor(df00, use="complete.obs"), n = nrow(df00))
pc.fit0 <- pc(suffStat,
               indepTest = gaussCItest, ## indep.test: partial correlations
               alpha=0.005, labels = names(df00))
# saveRDS(pc.fit0, "pcfit.rds")
plot(pc.fit0, main = "Estimated CPDAG")

# export pc.fit2 to bnlearn
res <- as.bn(pc.fit0@graph)
```

```{R}
library(igraph)
library(networkD3)

x <- igraph.from.graphNEL(as.graphNEL(res))

# Convert to object suitable for networkD3
net <- igraph_to_networkD3(x)
net$nodes$group <- c(1,1,1,1,2,2,2,2,3,3,4,4,4,4,4,4,3,3,3,5,5)
net$nodes$col <- c("yellow","yellow","yellow","yellow",
                         "brown","brown","brown","brown",
                         "blue","blue","pink","pink","pink","pink",
                         "pink","pink","blue","blue","blue","red","red")

# Create force directed network plot
forceNetwork(Links = net$links, Nodes = net$nodes, 
             Source = 'source', Target = 'target', 
             NodeID = 'name', Group = 'group')

src <- as.character(net$nodes$name[net$links$source + 1])
tgt <- as.character(net$nodes$name[net$links$target + 1])
simpleNetwork(Data = data.frame(src, tgt), fontSize = 16)
```

```{R}
pp <- graphviz.plot(x = res, highlight = list(nodes = nodes(res),
                                        arcs = arcs(res),
                                        col = "grey",
                                        textCol = "black"))
nodeRenderInfo(pp) <- list(col = c(),
                           textCol = c(),
                           fill = c("Y" = "grey", "M" = "grey", 
                                   "D" = "grey", "T" = "grey",
                                   "LAT" = "yellow", "LON" = "yellow",
                                   "ALT" = "yellow",
                                   "WS" = "pink", "WD" = "pink", 
                                   "TP" = "pink", "BLH" = "pink",
                                   "R" = "pink",
                                   "RID" = "orange",
                                   "O3" = "blue", "SO2" = "blue",
                                   "CO" = "blue", "PM10" = "blue",
                                   "PM25" = "blue", "NO2" = "blue",
                                   "C60" = "red", "L60" = "red"),
                           fontsize = 20)

# CLEAN UP EDGES
# TIME
pp@edgeL$Y$edges <- c(9, 13, 20)
pp@edgeL$D$edges <- c(9, 10, 11, 15, 17, 19)
pp@edgeL$M$edges <- c(11, 14, 17, 19)
pp@edgeL$T$edges
# GEO
pp@edgeL$LAT$edges <- c(13, 21)
pp@edgeL$LON$edges <- c(11, 16, 17, 18, 19, 20, 21)
pp@edgeL$ALT$edges
pp@edgeL$RID$edges <- c(9, 10, 16, 17, 21)
# HEALTH
pp@edgeL$C60$edges <- integer(0)
pp@edgeL$L60$edges <- integer(0)

renderGraph(pp)

edgeRenderInfo(pp) <- list(lty = c())

## show estimated CPDAG
library(igraph)
graphviz.plot(res)
plot(pc.fit0, main = "Estimated CPDAG", frame=TRUE, vertex.size=50)


# check and modify node ordering
undirected.arcs(res)
node.ordering(res)
ordering2blacklist(res)
res <- drop.arc(res, from = "latitude", to = "longitude")
res <- drop.arc(res, "latitude", "altitude")
res <- drop.arc(res, "LIV60", "CVD60")
ord = node.ordering(res)

bn.gs <- gs(df00[complete.cases(df00),], blacklist = ordering2blacklist(ord))
graphviz.plot(bn.gs)
score(x = bn.gs, data = df00, type = "bic-g")

node.ordering(res) <- c("LAT", "LON", "A", "so2", "o3", "co", "pm10", "pm2.5",
                        "no2", "TEMP", "WS", "WD", "BLH", "PREC", "C60", "L60")
```

# TEST BNLEARN, regardless of PC results
```{r}
library(bnlearn)

# Start SIMPLE!
# Use ws/wd modelled at the station (hourly)
df0 <- df[, -c(20, 23:27, 29, 39:42)]

df0 <- df[, c("Year", "Month", "Day", "Time", "code", "Region", "site.type",
             "latitude", "longitude", "altitude", "ws", "wd", "o3", "so2",
             "co", "pm10", "pm2.5", "no2", "TEMP", "BLH", "PREC", 
             "StCVDcancer60")]
df0 <- df0[complete.cases(df0),]
# or Use WS/WD modelled from ECMWF ERA INTERIM (daily)
df1 <- df[, c("Year", "Month", "Day", "Time", "code", "Region", "site.type",
             "latitude", "longitude", "altitude", 
             "o3", "so2", "co", "pm10", "pm2.5", "no2", 
             "WS", "WD", "TEMP", "BLH", "PREC", 
             "StCVDcancer60")]
df1 <- df1[complete.cases(df1),]

str(df1)
# saveRDS(df0, "UKdataset2Learn.rds")
```

Using bnlearn, test one-to-one dependecy with Pearson's correlation test (X2):
```{r}
ci.test("pm10", "TEMP", test="cor", data=df0)
ci.test("pm10", "PREC", test="cor", data=df0)
ci.test("pm10", "WS", test="cor", data=df0)
ci.test("pm10", "WD", test="cor", data=df0)
ci.test("pm10", "BLH", test="cor", data=df0)
```

Let's test the one-to-two hypothesis with Pearson's correlation test (X2):
```{r}
ci.test("pm10", "TEMP", "PREC", test="cor", data=df0)
```

# 
Use NO expert knowledge to define a Direct Acyclic Graph (DAG) using bnlearn.
Note that bnlearn's algorithms only work with complete cases!
```{r}
bn.mmhc <- mmhc(df0)
graphviz.plot(bn.mmhc)
# Network score (Bayesian Information Criterion)
score(x = bn.mmhc, data = df0)

saveRDS(bn.mmhc, "~/kehra/data/bn00.rds")
```

Topography cannot be influenced by climate/pollution factors, therefore we generate a black list (topography only) and update the BIC score:
```{r}
bl1 <- data.frame(from=c(rep(c("PM10","PM2p5","CO","O3","NO2","SO2","TEMP",
                               "WS", "WD","HMIX","PREC"), times=3),
                         c("Latitude", "Longitude"),
                         c("Latitude", "Z"),
                         c("Z", "Longitude")),
                  to=c(rep(c("Latitude", "Longitude", "Z"),each=11),
                       rep(c("Z", "Longitude", "Latitude"), each=2)))
bn.mmhc <- mmhc(df1, blacklist = bl1)
graphviz.plot(bn.mmhc)
score(x = bn.mmhc, data = df1, type = "bic-g")
```

Bayesian Networks do not allow for cyclic dependencies, therefore we assume that Climate affects Pollution and not viceversa. Therefore we generate black/white lists and update the BIC score:
```{r}
# black list
bl1 <- data.frame(from=c(rep("Latitude",2), rep("Longitude",2), rep("Z", 2),
                         rep(c("TEMP", "PREC", "WS", "WD", "HMIX",
                               "PM10", "PM2p5","CO", "O3", "NO2", "SO2"),3),
                         "WD", "WS", "PREC", "PM10", "PM10", "PM2p5", "PM2p5"),
                 to=c(c("Longitude", "Z"), c("Latitude", "Z"),
                      c("Longitude", "Latitude"),
                      rep("Latitude",11), rep("Longitude",11), rep("Z",11),
                      "WS", "WD", "WD","WS", "PM2p5", "PM10", "WD"))
# white list
wl1 <- data.frame(from=c("Z", "WS", "WS", "WD"),
                  to=c("TEMP", "PREC", "HMIX", "PM10"))

bn.mmhc <- mmhc(df1, whitelist = wl1, blacklist = bl1)
graphviz.plot(bn.mmhc)
score(x = bn.mmhc, data = df1, type = "bic-g")
```

## Study the conditional indenpendency
```{r}
# dseparation
nano <- nodes(bn.mmhc)
indNodes <- c()
for (n1 in nano){
  for (n2 in nano){
    if (dsep(bn.mmhc, n1, n2)){
      temp <- paste(n1, "and", n2, "are independent")
      indNodes <- c(indNodes, temp)
    }
  }
}
# What happens is I fix Temp?
nano <- nodes(bn.mmhc)
indNodes2 <- c()
fixedVar <- "WD" # "PREC" # "TEMP"
for (n1 in nano[nano != fixedVar]){
  for (n2 in nano[nano != fixedVar]){
    if (dsep(bn.mmhc, n1, n2)){
      temp <- paste(n1, "and", n2, "are independent")
      indNodes2 <- c(indNodes2, temp)
    }
  }
}

setdiff(indNodes, indNodes2)
setdiff(indNodes2, indNodes)
```

### TRAINING
Find local distributions and their parameters.
```{r}
fittedbn <- bn.fit(bn.mmhc, data = df0)
print(fittedbn)
print(fittedbn$pm10)
```

Note difference between fitted parameters for complete cases and those calculated using all observations!

```{r}
fittedbn$pm10
lm(pm10 ~ Time + latitude + wd + pm2.5, data=df0)               
confint(lm(pm10 ~ Time + latitude + wd + pm2.5, data=df0))

lm(pm10 ~ Time + latitude + wd + pm2.5, data=df)         # lm can handle NAs!  
confint(lm(pm10 ~ Time + latitude + wd + pm2.5, data=df)) 

quantile(df0$pm10, na.rm = TRUE) # using observed CO and WD
quantile(predict(lm(pm10 ~ Time + latitude + wd + pm2.5, data=df0)))     # using observed vaues
```

# INFERENCE

The first column is the vector of marginal expectations, the second is the marginal standard deviation and the remaining is the correlation matrix.

E.g. What values of SO2 should I expect if PM10 raises above EU limits?
```{r}
fittedbn$pm10
fittedbn$so2
fittedbn$CVD60

# observed PM10 and others
quantile(df0$pm10) # quantile(df$PM10, na.rm = TRUE)
quantile(df0$so2)
quantile(df0$wd)

# how is the model doing trying to predict PM10 knowing the appropriate range for SO2?
newPM10 <- cpdist(fittedbn, nodes = "pm10", evidence = (so2 < 300))
quantile(newPM10[,1])

# how is the model doing trying to predict PM10 knowing the appropriate range for LAT?
newPM10 <- cpdist(fittedbn, nodes = "pm10", evidence = (latitude < 54))
quantile(newPM10[,1])

# What is the probability of SO2 > 300, if PM10 > 50?
cpquery(fittedbn, event = (so2 > 300), evidence = (pm10 > 50))
cpquery(fittedbn, event = (so2 > 300), evidence = (pm10 < 50))

cpquery(fittedbn, event = (so2 > 300), evidence = (pm2.5 < 40))
cpquery(fittedbn, event = (so2 > 300), evidence = (pm2.5 > 40))

quantile(df0$CVD60)
cpquery(fittedbn, event = (CVD60 > 10), evidence = (pm2.5 > 40))
```


